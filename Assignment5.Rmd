---
title: 'FinStat: Assignmnet 5'
author: "Matthew Donaldson"
date: "3/1/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 4.7
**1)** 
As seen from the plot below, a linear regression model would be reasonable to describe the data because the trend looks linear with a negative slope.
```{r}

strData = Rsafd::STRENGTH
#head(strData)
#dim(strData)
plot(strData)


```

**2)**
```{r}

str_fit = lm(strData[,2]~strData[,1])
summary(str_fit)
#plot(strData)
#abline(str_fit$coefficients)
#sprintf("Variance as R^2: %f" , summary(str_fit)$r.squared)
sprintf('Variance of the noise: %f', sigma(str_fit)^2)

```

**3)**
As seen from the R^2 value above in the summary fit, 0.89. Since the R^2 high, it shows a good fit of the model. Note the first graph is the fitted values and the actual vales of attenuation plotted on the same graph where the fitted values are the black circles. The second graph is the fitted values vs the actual values.Lastly, an output is shown for r^2 and the predicted values for x = 20, 50 ,75.
```{r}

newdata = data.frame(strData[,1])
predictions = predict(str_fit,newdata,interval = "prediction")
#head(predictions)

plot(strData)
abline(str_fit$coefficients)
points(strData[,1], predictions[,1], pch = 19)

plot(strData[,2], predictions[,1])
sprintf("R^2 Coefficient: %f" , summary(str_fit)$r.squared)


#newdata = data.frame(strength = c(20, 50, 75))
#new_pred = predict(str_fit,newdata = newdata,interval = "prediction")
#new_pred
x = c(20, 50 ,75)
b0 = str_fit$coefficients[1]
b1 = str_fit$coefficients[2]

y_pred = b0 + b1 %*% x
y_pred
```
**4)**
From the qqnorm plot it can be seen that the residuals follows a normal distributions.
This can be interpreted as the noise of the data set to be gaussian white noise 
with mean 0 and variance $\sigma^2$.
```{r}

#res = (y_pred - strData[,2])

resid = str_fit$residuals
plot(resid,predictions[,1])

qqnorm(resid)


```
**5)**
In the output, you can see the slope and intercept. 
```{r}
library(Rsafd)
x = strData[,1]
y = strData[,2]
# Part 1
abs_fit = l1fit(x, y)

abs_fit$coef
y_hat = abs_fit$coef[1] + abs_fit$coef[2] %*% x



plot(x,y_hat)

SSE = sum((abs_fit$residuals)^2)
variance = SSE/(length(x)-2)
sprintf('Variance of noise: %f', variance)

mean_y = mean(abs_fit$fitted.values)

TSS = sum((y-mean_y)^2)

R2 = 1 - SSE/TSS
sprintf('R squared: %f', R2)
```






## Problem 4.9
```{r}
#1.1
baskData = Rsafd::BASKETBALL
head(baskData)
```
```{r}
baskHeight_fit = lm(baskData$points~baskData$height)

summary(baskHeight_fit)

plot(baskData$height, baskData$points)
abline(baskHeight_fit$coefficients)
```

```{r}
#1.2
baskMin_fit = lm(baskData$points~baskData$minutes)
summary(baskMin_fit)

plot(baskData$minutes, baskData$points)
abline(baskMin_fit$coefficients)


```
```{r}
#1.3

baskAge_fit = lm(baskData$points~baskData$age)
summary(baskAge_fit)

plot(baskData$age, baskData$points)
abline(baskAge_fit$coefficients)


```
**4)** 
As seen from the r^2 and f-statistic it shows that minutes is able to predict the points the most because it has the highest r^2 value and a large F-statistic saying we would expect a certain slope. Then the second best is height, with an r^2 value of 0.06 and a descent F-statistic. Lastly, age is not good at predicting points which can be seen from a low r^2 and F-statistic value.
```{r}
#1.4
summary(baskAge_fit)$r.squared
summary(baskMin_fit)$r.squared
summary(baskHeight_fit)$r.squared

summary(baskAge_fit)$fstat[1]
summary(baskMin_fit)$fstat[1]
summary(baskHeight_fit)$fstat[1]




```

**2)**
Age does not improve the model as seen from the fact that the r^2 value does not change much as well as the F-statistic decreasing when adding the age in the regression. Lastly, looking at the t-test done in the model with age, it shows that the variables slope is not significant. 
```{r}

fit1 = lm(baskData$points ~ baskData$height + baskData$minutes)
summary(fit1)

fit2 = lm(baskData$points ~ baskData$height + baskData$minutes + baskData$age)
summary(fit2)


```

**3)**
 The coefficients are shown below.
```{r}
#part 1,2,3
l1H_fit = l1fit(baskData$height, baskData$points)
l1M_fit = l1fit(baskData$minutes, baskData$points)
l1A_fit = l1fit(baskData$age, baskData$points)

l1H_fit$coefficients
l1M_fit$coefficients
l1A_fit$coefficients

 


```
The R2 value show that all of them are not as good of a fit for predicting points. This is different from the OLS fit regression. I think this is because absolute value fit is less effected by outliers.

```{r}
y = baskData$points
SSE_H = sum((l1H_fit$residuals)^2)
SSE_M = sum((l1M_fit$residuals)^2)
SSE_A = sum((l1A_fit$residuals)^2)

mean_y = mean(baskData$points)


TSS_H = sum((y-mean_y)^2)
TSS_M = sum((y-mean_y)^2)
TSS_A = sum((y-mean_y)^2)

R2_H = 1 - SSE_H/TSS_H
R2_M = 1 - SSE_M/TSS_M
R2_A = 1 - SSE_A/TSS_A

R2_H
R2_M
R2_A
```








